import re
import scipy
import numpy as np
import pandas as pd
from nltk.corpus import stopwords
from textblob import Word

import spacy
from spacy_wordnet.wordnet_annotator import WordnetAnnotator

# Loading necessary packages
import nltk

nltk.download('stopwords')
nltk.download('wordnet')

# Load an spacy model (supported models are "es" and "en")
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')

# Path of word embedding pretrained model glove 6B tokens, 400K vocab and  50d /  300d vectors
gloveFile = "data/glove.6B.50d.txt"


# gloveFile = "data/glove.6B.300d.txt"


# Loading glove pretrained model for perform word embedding
def loadGloveModel(gloveFile_d):
    print("Loading Glove Model")
    with open(gloveFile_d, encoding="utf8") as f:
        content = f.readlines()
    model_ = {}
    for line in content:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model_[word] = embedding
    print("Done.", len(model_), " words loaded!")
    return model_


# Loading pretrained glove model
model = loadGloveModel(gloveFile)


def get_similar_prases(phrase_1):
    # Cleaning text data
    def preprocess(raw_text):
        # keeping only english words
        letters_only_text = re.sub("[^a-zA-Z]", " ", raw_text)

        # convert to lower case and split
        words = letters_only_text.lower().split()

        # remove stopwords
        stopword_set = set(stopwords.words("english"))
        cleaned_words = list(set([w for w in words if w not in stopword_set]))

        return cleaned_words

    # Calculating cosine distance between two sentences
    def cosine_distance_wordembedding_method(s1, s2):
        # Vector representation of fisrt sentence
        vector_1 = np.mean([model[word] for word in preprocess(s1)], axis=0)

        # Vector representation of second sentence
        vector_2 = np.mean([model[word] for word in preprocess(s2)], axis=0)

        # Cosine distance of two sentence
        cosine = scipy.spatial.distance.cosine(vector_1, vector_2)

        # print('Two sentences are similar to',round((1-cosine)*100,2),'%')
        return 1 - cosine

    # Similar and related word generated by taking synonyms and domain of word
    def combination_phrase(test_word):
        # print(test_word)
        each_word = test_word
        each_word = each_word.lower()

        text_word = Word(each_word)
        synonyms = set()
        for synset in text_word.synsets:
            for lemma in synset.lemmas():
                synonyms.add(lemma.name())

        if len(synonyms) > 0:
            syn_words = [i for i in synonyms]
        else:
            synonyms.add(each_word)
            syn_words = [i for i in synonyms]

        temp_dict_1 = similar_generated_word(test_word, syn_words)
        token = nlp(test_word)[0]
        domain = token._.wordnet.wordnet_domains()
        temp_dict_1.update(similar_generated_word(test_word, domain))
        # print(temp_dict_1)
        return temp_dict_1

    # Filter only most related word by taking cosine distance between two words
    def similar_generated_word(test_word, words):
        def listToString(s):
            str1 = ""
            for ele in s:
                str1 = str1 + ele + " "
            return str1

        similar_word = {}
        vector_1 = model[test_word]
        for each_d in words:
            try:
                if "_" in each_d:
                    ph = each_d.split("_")
                    phr = listToString(ph)
                    vector_2 = np.mean([model[word] for word in preprocess(phr)], axis=0)
                else:
                    vector_2 = model[each_d]
                cosine = scipy.spatial.distance.cosine(vector_1, vector_2)

                if (1 - cosine) > 0.3:
                    similar_word[each_d] = 1 - cosine
            except:
                continue
        return similar_word

    # Sentence generated by taking all combination of related words
    def sentence_combination(words_list):
        past_info_array = []
        sentences = []
        x = words_list[0]
        for k in range(0, len(words_list) - 1):
            y = words_list[k + 1]
            for i in range(len(x)):
                for j in range(len(y)):
                    past_info_array.append(x[i] + " " + y[j])
                    temp = x[i] + " " + y[j]
                    if len(temp.split(" ")) == len(words_list):
                        sentences.append(x[i] + " " + y[j])

            if len(words_list) > (k + 1):
                max_len = max([len(i.split(" ")) for i in past_info_array])
                past_info_array = [i for i in past_info_array if len(i.split(" ")) == max_len]
                x = past_info_array

        return sentences

    # Sentence generation
    def senetence_generation(sentence):
        words = []
        for word in sentence.split(" "):
            word = word.lower()
            words.append(combination_phrase(word))
        # print(words)
        words_list = []
        for each_dict in words:
            words_list.append(list(each_dict.keys()))

        similar_sent = sentence_combination(words_list)

        return similar_sent

    # removing punctuation
    def remove_punctuation(text):
        def punct_numbers(string):
            punc_pattern = re.compile("["
                                      u"\U00000021-\U0000002F"  # !"#$%&\'()*+,-./
                                      u"\U0000003A-\U00000040"  # :;<=>?
                                      u"\U0000005B-\U00000060"  # [\\]^_`
                                      u"\U0000007B-\U0000007F"  # {|}~
                                      u"\U00000030-\U00000039"  # 0-9
                                      "]+", flags=re.UNICODE)
            return punc_pattern.sub(r' ', string)

        text = punct_numbers(text)
        return text

    # text = remove_punctuation(s_text)

    # removing underscore from from phrases
    def remove_uderscore(org_simi_sent):
        modified = dict()
        vals = []
        for each in list(org_simi_sent.values())[0]:
            each_p = each[0].replace("_", " ")
            each_p = each_p.capitalize()
            vals.append([each_p, each[1]])
        modified[list(org_simi_sent.items())[0][0]] = vals
        return modified

    # Text/phrase similarity on provided dataset
    def simi_sent_generation(phrase_df_):
        def noise_remove(s):
            s = [i for i in s.split(" ") if i.isalpha()]
            str1 = ""
            for ele in s:
                if ele != s[-1]:
                    str1 = str1 + ele + " "
                else:
                    str1 = str1 + ele
            return str1

            # Finding local minima thresold value to get closely related sentence

        def thresold_minuma_valued_sent(ss1_, similar_sent_):
            def related_sent(ss1_s_, similar_sent_s_, thresold_):
                most_related_sent_s_ = []
                for each_gen_sent in similar_sent_s_:
                    ss2 = each_gen_sent
                    inv_cosine_distance = cosine_distance_wordembedding_method(ss1_s_, ss2)
                    if inv_cosine_distance > thresold_:
                        if ss1_s_.lower() != ss2:
                            sd = round(inv_cosine_distance * 100, 2)
                            percentage = "Similarity: " + str(sd) + '%'
                            most_related_sent_s_.append([ss2, percentage])
                return most_related_sent_s_

            thresold = 0.88
            flag = 14
            # print(most_related_sent_s_)
            while 0.75 < thresold <= 1 and flag > 0:
                # flag == flag - 1
                flag = flag - 1
                # print(flag)
                most_related_sent_ = related_sent(ss1_, similar_sent_, thresold)
                # print(most_related_sent_s_)
                if 7 >= len(most_related_sent_) >= 2:
                    break

                if len(most_related_sent_) > 7:
                    thresold = thresold + 0.01

                if len(most_related_sent_) <= 0:
                    thresold = thresold - 0.01
                # print(thresold_)

            print(thresold)
            most_related_sent_ = related_sent(ss1_, similar_sent_, thresold)
            return most_related_sent_

        most_related_sent_dict = {}
        for c, p_each_phrase in enumerate(phrase_df_["Sample phrase"]):
            # p_each_phrase = 'high efficiency fuel pump solutions'
            print(p_each_phrase)
            # most_related_sent_s_ = []
            ss1 = remove_punctuation(p_each_phrase)
            ss1 = noise_remove(ss1)
            similar_sent = senetence_generation(ss1)
            most_related_sent = thresold_minuma_valued_sent(ss1, similar_sent)
            most_related_sent_dict[p_each_phrase] = most_related_sent
            # break
        return most_related_sent_dict

    # Loading dataset
    # phrase_df = pd.read_excel('data/ideapoke_poc_data.xlsx', sheet_name="Phrase")
    # phrase_df.head()

    phrase_df = pd.DataFrame(data=[phrase_1], columns=['Sample phrase'])
    org_simi_sent = remove_uderscore(simi_sent_generation(phrase_df))
    print(org_simi_sent)

    return org_simi_sent
